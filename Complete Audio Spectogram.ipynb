{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\PC\\Anaconda3\\envs\\TensorFlow-GPU-Keras\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "import model\n",
    "import preprocessing\n",
    "import utils\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = utils.yaml_to_dict('config.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir el nuevo conjunto de datos\n",
    "\n",
    "Este dataset contiene el Espectograma de la palabra completa, osea que no se le aplica el proceso de separaci√≥n por ventanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(spectogram,specgram_folder,file):\n",
    "    \n",
    "    if not os.path.exists(specgram_folder):\n",
    "        os.makedirs(specgram_folder)\n",
    "      \n",
    "    fpath = os.path.join(specgram_folder, file)\n",
    "    im = Image.fromarray(spectogram,'L')\n",
    "    im.save(fpath)\n",
    "\n",
    "def generate_spectogram_images(params):\n",
    "    \n",
    "    shape_list = list()\n",
    "    \n",
    "    audios_path = os.path.join(params['data_dir'],params['data_dir_audios'])\n",
    "    \n",
    "    for root, dirs, files in os.walk(audios_path, topdown=False):\n",
    "        for name in files:\n",
    "            \n",
    "            specgram_folder = os.path.join(params['data_dir'],params['data_dir_images']+'_test',root.split(os.path.sep)[-1])\n",
    "        \n",
    "            [fs, x] = audioBasicIO.readAudioFile(os.path.join(root, name))\n",
    "            x = audioBasicIO.stereo2mono(x)\n",
    "            x = preprocessing._rescaled_signal(x)\n",
    "            \n",
    "            segments = preprocessing._find_segments_from_audio(x=x, fs=fs)\n",
    "            \n",
    "            for i,segment in enumerate(segments):\n",
    "                \n",
    "                imname = 'specgram_matrix_' + os.path.splitext(name)[0] + '_segment' + str(i) + '.png'\n",
    "                audio_fragment = preprocessing._extract_audio_fragments(x=x, fs=fs, segment=segment)\n",
    "\n",
    "                specgram, TimeAxis, FreqAxis = audioFeatureExtraction.stSpectogram(\n",
    "                    audio_fragment,\n",
    "                    fs,\n",
    "                    round(fs * 0.02),\n",
    "                    round(fs * 0.01),\n",
    "                    False)\n",
    "                \n",
    "                shape_list.append(specgram.shape)\n",
    "            \n",
    "                save_img(specgram,specgram_folder,imname)\n",
    "                \n",
    "    return shape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_list = generate_spectogram_images(params)\n",
    "w, h = np.mean(s_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = int(w)\n",
    "#h = int(h)\n",
    "w = 110\n",
    "h = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['data_dir_images'] = 'images_test'\n",
    "preprocessing.split_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 55, 240, 64)       9472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 55, 240, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 55, 240, 64)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 10,378\n",
      "Trainable params: 10,250\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(w, h, 3))\n",
    "net = model.ModelArchitecture(num_classes=params['num_classes'])\n",
    "x = net(inputs, training=False)\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear el generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_datagenerator(params,mode):  \n",
    "    \n",
    "    data_path = os.path.join(params['data_dir'],params['data_dir_images'],mode)\n",
    "\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=(w,h),\n",
    "        batch_size=params['batch_size'],\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    return train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in train\n",
      "Found 3050 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Data in validation')\\ntrain_generator = data.make_datagenerator(params,mode='training')\\nprint('Data in validation')\\nval_generator = data.make_datagenerator(params,mode='validation')\\n\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Data in train')\n",
    "train_generator = mk_datagenerator(params)\n",
    "\"\"\"\n",
    "print('Data in validation')\n",
    "train_generator = data.make_datagenerator(params,mode='training')\n",
    "print('Data in validation')\n",
    "val_generator = data.make_datagenerator(params,mode='validation')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_id_map = preprocessing._load_label_id_map(params)\n",
    "label_map = {v:k for (k, v) in label_id_map.items()}\n",
    "\n",
    "fig, axis = plt.subplots(2,5, figsize=(15,15), constrained_layout=True)\n",
    "iteration = next(train_generator)\n",
    "size = len(iteration[0])\n",
    "k = 0\n",
    "for i in range(2):\n",
    "    for j in range(size//2):\n",
    "        \n",
    "        spectogram = iteration[0][k]\n",
    "        label = label_map[np.argmax(iteration[1][k])]\n",
    "    \n",
    "        axis[i][j].imshow(spectogram)\n",
    "        axis[i][j].title.set_text(label)\n",
    "\n",
    "        axis[i][j].get_xaxis().set_visible(False)\n",
    "        axis[i][j].get_yaxis().set_visible(False)\n",
    "        \n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(params['model_dir'], 'tf_ckpt'), \n",
    "    save_weights_only=True, \n",
    "    verbose=1,\n",
    "    period=5)\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    os.path.join(params['model_dir'], 'logs'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.00001)\n",
    "\n",
    "steps_per_epoch = train_generator.n // params['batch_size']\n",
    "#validation_steps = val_generator.n // params['batch_size']\n",
    "\n",
    "net.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "net.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=params['num_epochs'],\n",
    "    workers=4,\n",
    "    #validation_data=val_generator, \n",
    "    #validation_steps=validation_steps,\n",
    "    callbacks=[cp_callback, tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = mk_datagenerator(params,'train')\n",
    "val_generator = mk_datagenerator(params,'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 5s 108ms/step - loss: 7.1699 - acc: 0.5480 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 8.0393 - acc: 0.4994 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 9.1594 - acc: 0.4317 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 9.1916 - acc: 0.4297 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 8.5604 - acc: 0.4689 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00005: saving model to checkpoints\\tf_ckpt\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 6s 115ms/step - loss: 8.9660 - acc: 0.4437 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 9.2374 - acc: 0.4269 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 8.7267 - acc: 0.4586 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 8.9524 - acc: 0.4446 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 8.7682 - acc: 0.4560 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00010: saving model to checkpoints\\tf_ckpt\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 9.4767 - acc: 0.4120 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 8.7725 - acc: 0.4557 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 8.7725 - acc: 0.4557 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 9.1459 - acc: 0.4326 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 8.7454 - acc: 0.4574 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00015: saving model to checkpoints\\tf_ckpt\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 8.8744 - acc: 0.4494 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 8.8609 - acc: 0.4503 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 9.1781 - acc: 0.4306 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 8.9294 - acc: 0.4460 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 9.1916 - acc: 0.4297 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00020: saving model to checkpoints\\tf_ckpt\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 5s 100ms/step - loss: 8.5791 - acc: 0.4677 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 9.1085 - acc: 0.4349 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 8.8744 - acc: 0.4494 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 9.1594 - acc: 0.4317 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      " 3/50 [>.............................] - ETA: 7s - loss: 5.3727 - acc: 0.6667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\envs\\TensorFlow-GPU-Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.295228). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\PC\\Anaconda3\\envs\\TensorFlow-GPU-Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.162587). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 5s 94ms/step - loss: 8.8557 - acc: 0.4506 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00025: saving model to checkpoints\\tf_ckpt\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 9.2748 - acc: 0.4246 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 8.5468 - acc: 0.4697 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 8.9389 - acc: 0.4454 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 29/30\n",
      " 4/50 [=>............................] - ETA: 16s - loss: 7.6561 - acc: 0.5250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\envs\\TensorFlow-GPU-Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.241897). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\PC\\Anaconda3\\envs\\TensorFlow-GPU-Keras\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.627743). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 5s 104ms/step - loss: 9.0584 - acc: 0.4380 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 9.0627 - acc: 0.4377 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00030: saving model to checkpoints\\tf_ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24aaedff470>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "input_shape = (w, h, 3)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2)) \n",
    "model.add(Activation('softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "steps_per_epoch = 50#train_generator.n // params['batch_size']\n",
    "validation_steps = 50#val_generator.n // params['batch_size']\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(params['model_dir'], 'tf_ckpt'), \n",
    "    save_weights_only=True, \n",
    "    verbose=1,\n",
    "    period=5)\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    os.path.join(params['model_dir'], 'logs'))\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=params['num_epochs'],\n",
    "    workers=4,\n",
    "    validation_data=val_generator, \n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[cp_callback, tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
