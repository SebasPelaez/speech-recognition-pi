{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/martin/anaconda3/envs/pi2/lib/python3.7/site-packages/pydub/utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "import model\n",
    "import preprocessing\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = utils.yaml_to_dict(os.path.join('..','config.yml'))\n",
    "params['data_dir'] = os.path.join('..',params['data_dir'])\n",
    "params['model_dir'] = os.path.join('..',params['model_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer paso: preprocesar los datos \n",
    "Hacerlo solo una vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size is bigger than record segment: specgram_matrix_S-avanza9_segment22.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-izquierda7_segment6.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-izquierda9_segment25.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-frena10_segment24.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-frena5_segment28.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-acelera11_segment24.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-acelera8_segment0.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-acelera8_segment1.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-acelera7_segment16.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-acelera6_segment15.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-acelera5_segment11.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-acelera4_segment0.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede5_segment0.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede4_segment0.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede11_segment6.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede11_segment14.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede11_segment16.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede11_segment26.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment0.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment2.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment3.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment5.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment6.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment11.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment13.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment14.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment15.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment17.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment21.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-retrocede8_segment23.png\n",
      "Window size is bigger than record segment: specgram_matrix_S-arranca8_segment0.png\n"
     ]
    }
   ],
   "source": [
    "preprocessing.download_data(params)\n",
    "preprocessing.extract_data(params)\n",
    "preprocessing.generate_spectogram_images(params)\n",
    "preprocessing.make_id_label_map(params)\n",
    "preprocessing.split_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segund Paso: instanciar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 10:28:17.901544 140469068961600 deprecation.py:506] From /home/martin/anaconda3/envs/pi2/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_architecture\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 55, 240, 3)        444       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 55, 240, 3)        12        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 55, 240, 3)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 11, 48, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1584)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                19020     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                130       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 19,654\n",
      "Trainable params: 19,624\n",
      "Non-trainable params: 30\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(110,480, 3))\n",
    "net = model.ModelArchitecture(num_classes=params['num_classes'])\n",
    "x = net(inputs, training=False)\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tercer paso: crear el generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in train\n",
      "Found 2959 images.\n",
      "Data in validation\n",
      "Found 30 images.\n"
     ]
    }
   ],
   "source": [
    "print('Data in train')\n",
    "train_generator = data.make_datagenerator(params, mode='training')\n",
    "print('Data in validation')\n",
    "val_generator = data.make_datagenerator(params,mode='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuarto paso: correr el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 10:28:18.400288 140469068961600 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "295/295 [==============================] - 21s 72ms/step - loss: 2.7561 - sparse_categorical_accuracy: 0.1387 - val_loss: 2.1842 - val_sparse_categorical_accuracy: 0.2333\n",
      "Epoch 2/25\n",
      "295/295 [==============================] - 24s 82ms/step - loss: 1.8185 - sparse_categorical_accuracy: 0.3320 - val_loss: 1.4793 - val_sparse_categorical_accuracy: 0.5667\n",
      "Epoch 3/25\n",
      "295/295 [==============================] - 23s 77ms/step - loss: 1.2722 - sparse_categorical_accuracy: 0.6053 - val_loss: 1.0935 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 4/25\n",
      "295/295 [==============================] - 22s 73ms/step - loss: 0.9757 - sparse_categorical_accuracy: 0.7860 - val_loss: 0.8452 - val_sparse_categorical_accuracy: 0.7667\n",
      "Epoch 5/25\n",
      "294/295 [============================>.] - ETA: 0s - loss: 0.8047 - sparse_categorical_accuracy: 0.8408\n",
      "Epoch 00005: saving model to ../checkpoints/tf_ckpt\n",
      "295/295 [==============================] - 22s 75ms/step - loss: 0.8054 - sparse_categorical_accuracy: 0.8410 - val_loss: 0.7005 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 6/25\n",
      "295/295 [==============================] - 21s 70ms/step - loss: 0.7105 - sparse_categorical_accuracy: 0.8776 - val_loss: 0.6062 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 7/25\n",
      "295/295 [==============================] - 21s 70ms/step - loss: 0.6318 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.5762 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 8/25\n",
      "295/295 [==============================] - 27s 91ms/step - loss: 0.6046 - sparse_categorical_accuracy: 0.8973 - val_loss: 0.5615 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 9/25\n",
      "295/295 [==============================] - 25s 84ms/step - loss: 0.5559 - sparse_categorical_accuracy: 0.9084 - val_loss: 0.4644 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 10/25\n",
      "294/295 [============================>.] - ETA: 0s - loss: 0.5561 - sparse_categorical_accuracy: 0.9092\n",
      "Epoch 00010: saving model to ../checkpoints/tf_ckpt\n",
      "295/295 [==============================] - 23s 77ms/step - loss: 0.5565 - sparse_categorical_accuracy: 0.9088 - val_loss: 0.5073 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 11/25\n",
      "295/295 [==============================] - 21s 70ms/step - loss: 0.5360 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.5863 - val_sparse_categorical_accuracy: 0.8333\n",
      "Epoch 12/25\n",
      "295/295 [==============================] - 22s 75ms/step - loss: 0.5059 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.5726 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 13/25\n",
      "295/295 [==============================] - 25s 84ms/step - loss: 0.5077 - sparse_categorical_accuracy: 0.9152 - val_loss: 0.5464 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 14/25\n",
      "295/295 [==============================] - 23s 80ms/step - loss: 0.4849 - sparse_categorical_accuracy: 0.9254 - val_loss: 0.4910 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 15/25\n",
      "294/295 [============================>.] - ETA: 0s - loss: 0.4745 - sparse_categorical_accuracy: 0.9231\n",
      "Epoch 00015: saving model to ../checkpoints/tf_ckpt\n",
      "295/295 [==============================] - 21s 71ms/step - loss: 0.4737 - sparse_categorical_accuracy: 0.9234 - val_loss: 0.4573 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 16/25\n",
      "295/295 [==============================] - 20s 68ms/step - loss: 0.4794 - sparse_categorical_accuracy: 0.9213 - val_loss: 0.4357 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 17/25\n",
      "295/295 [==============================] - 20s 67ms/step - loss: 0.4710 - sparse_categorical_accuracy: 0.9237 - val_loss: 0.4193 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 18/25\n",
      "295/295 [==============================] - 20s 67ms/step - loss: 0.4583 - sparse_categorical_accuracy: 0.9278 - val_loss: 0.4028 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 19/25\n",
      "295/295 [==============================] - 22s 76ms/step - loss: 0.4447 - sparse_categorical_accuracy: 0.9278 - val_loss: 0.4641 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 20/25\n",
      "294/295 [============================>.] - ETA: 0s - loss: 0.4527 - sparse_categorical_accuracy: 0.9244\n",
      "Epoch 00020: saving model to ../checkpoints/tf_ckpt\n",
      "295/295 [==============================] - 22s 76ms/step - loss: 0.4534 - sparse_categorical_accuracy: 0.9240 - val_loss: 0.4124 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 21/25\n",
      "295/295 [==============================] - 21s 72ms/step - loss: 0.4384 - sparse_categorical_accuracy: 0.9271 - val_loss: 0.3781 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 22/25\n",
      "295/295 [==============================] - 21s 70ms/step - loss: 0.4277 - sparse_categorical_accuracy: 0.9329 - val_loss: 0.4202 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 23/25\n",
      "295/295 [==============================] - 23s 76ms/step - loss: 0.4191 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.5235 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 24/25\n",
      "295/295 [==============================] - 26s 87ms/step - loss: 0.4257 - sparse_categorical_accuracy: 0.9268 - val_loss: 0.3819 - val_sparse_categorical_accuracy: 0.9333\n",
      "Epoch 25/25\n",
      "294/295 [============================>.] - ETA: 0s - loss: 0.4253 - sparse_categorical_accuracy: 0.9326\n",
      "Epoch 00025: saving model to ../checkpoints/tf_ckpt\n",
      "295/295 [==============================] - 24s 82ms/step - loss: 0.4248 - sparse_categorical_accuracy: 0.9329 - val_loss: 0.4392 - val_sparse_categorical_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc116ce25f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(params['model_dir'], 'tf_ckpt'), \n",
    "    save_weights_only=True, \n",
    "    verbose=1,\n",
    "    period=5)\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    os.path.join(params['model_dir'], 'logs'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "steps_per_epoch = train_generator.n // params['batch_size']\n",
    "validation_steps = val_generator.n // params['batch_size']\n",
    "\n",
    "net.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "net.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=params['num_epochs'],\n",
    "    workers=4,\n",
    "    validation_data=val_generator, \n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[cp_callback,tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fpr, tpr, thresholds = roc_curve(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quintero paso: evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[0 0 1 0 0 0 0 1 1 0]\n",
      " [1 0 0 0 1 1 0 0 0 1]\n",
      " [1 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 0 1 0 1 0 0 1]\n",
      " [1 1 0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 2]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     arranca       0.00      0.00      0.00         3\n",
      "     acelera       0.00      0.00      0.00         4\n",
      "        pita       0.25      0.33      0.29         3\n",
      "   izquierda       0.00      0.00      0.00         2\n",
      "       frena       0.00      0.00      0.00         4\n",
      "     detente       0.00      0.00      0.00         3\n",
      "   retrocede       0.00      0.00      0.00         3\n",
      "        gira       0.00      0.00      0.00         2\n",
      "      avanza       0.00      0.00      0.00         2\n",
      "     derecha       0.33      0.50      0.40         4\n",
      "\n",
      "   micro avg       0.10      0.10      0.10        30\n",
      "   macro avg       0.06      0.08      0.07        30\n",
      "weighted avg       0.07      0.10      0.08        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "Y_pred = net.predict_generator(val_generator, validation_steps)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(val_generator.data, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Cats', 'Dogs', 'Horse']\n",
    "print(classification_report(val_generator.data, y_pred, target_names=[\"arranca\", \"acelera\", \"pita\", \"izquierda\",\n",
    "                                                                      \"frena\", \"detente\", \"retrocede\", \"gira\", \"avanza\", \"derecha\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi2",
   "language": "python",
   "name": "pi2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
