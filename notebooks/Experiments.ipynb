{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "import model\n",
    "import preprocessing\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se definen las configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rates = [0.001, 0.01]\n",
    "epochs = [10,20,30]\n",
    "batch_sizes = [8,16,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 1 #Número donde inician los experimentos\n",
    "for batch in batch_sizes:\n",
    "    for epoch in epochs:\n",
    "        for learning_rate in l_rates:\n",
    "        \n",
    "            print(\"*\"*20)\n",
    "            print(\"EXPERIMENTO: Batch:\", batch, \"Epoc: \", epoch, \"learning_rate: \", learning_rate)\n",
    "            \n",
    "            #Cargar los parametros cada que inicia un experimento\n",
    "            params = utils.yaml_to_dict(os.path.join('..','config.yml'))\n",
    "            params['data_dir'] = os.path.join('..',params['data_dir'])\n",
    "            \n",
    "            experiment_name = 'Experimento10-Configuracion'+str(experiment)\n",
    "            params['model_dir'] = os.path.join('..','experiments-weights',experiment_name)\n",
    "            params['batch_size'] = batch\n",
    "            params['num_epochs'] = epoch\n",
    "            params['learning_rate'] = learning_rate\n",
    "                        \n",
    "            width, height = params[\"image_shape\"]\n",
    "            \n",
    "            #Limpiar Sesión\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            #Cargar el modelo\n",
    "            inputs = tf.keras.layers.Input(shape=(width,height,3))\n",
    "            net = model.ModelArchitecture(num_classes=params['num_classes'])\n",
    "            x = net(inputs, training=False)\n",
    "            \n",
    "            #Generar conjuntos de entrenamiento y validación y llevarlos a un generador\n",
    "            preprocessing.split_data(params)\n",
    "            train_generator = data.make_datagenerator(params, mode='training')\n",
    "            val_generator = data.make_datagenerator(params,mode='validation')\n",
    "            \n",
    "            #Crear el directorio que guarda los checkpoints\n",
    "            if not os.path.exists(params['model_dir']):\n",
    "                os.makedirs(params['model_dir'])\n",
    "\n",
    "            #Definir los callbacks\n",
    "            cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                os.path.join(params['model_dir'], 'tf_ckpt'), \n",
    "                save_weights_only=True, \n",
    "                verbose=1,\n",
    "                period=5)\n",
    "\n",
    "            tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "                os.path.join(params['model_dir'], 'logs'))\n",
    "\n",
    "            #Definir la función de optimización\n",
    "            optimizer = tf.keras.optimizers.Adam(params['learning_rate'])\n",
    "\n",
    "            #Definir los pasos para entrenamiento y validación\n",
    "            steps_per_epoch = train_generator.n // params['batch_size']\n",
    "            validation_steps = val_generator.n // params['batch_size']\n",
    "\n",
    "            #Entrenar el modelo\n",
    "            net.compile(optimizer=optimizer, loss=params['loss'], metrics=['sparse_categorical_accuracy'])\n",
    "            net.fit_generator(\n",
    "                train_generator, \n",
    "                steps_per_epoch=steps_per_epoch, \n",
    "                epochs=params['num_epochs'],\n",
    "                workers=4,\n",
    "                validation_data=val_generator, \n",
    "                validation_steps=validation_steps,\n",
    "                callbacks=[cp_callback,tb_callback])\n",
    "            \n",
    "    \n",
    "            #Obtener las métricas de resultados en el conjunto de validación\n",
    "            params['batch_size'] = val_generator.n\n",
    "            params['shuffle'] = False\n",
    "            val_generator = data.make_datagenerator(params,mode='validation')\n",
    "            Y_pred = net.predict_generator(val_generator, steps=1)\n",
    "            y_pred = np.argmax(Y_pred, axis=1)\n",
    "            \n",
    "            print(confusion_matrix(val_generator.data, y_pred))\n",
    "            target_names = [\"arranca\", \"acelera\", \"pita\", \"izquierda\",\"frena\", \"detente\", \"retrocede\", \"gira\", \"avanza\", \"derecha\"]\n",
    "            print(classification_report(val_generator.data, y_pred, target_names=target_names))\n",
    "            \n",
    "            #Aumentar el contador de experimentos\n",
    "            experiment += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
